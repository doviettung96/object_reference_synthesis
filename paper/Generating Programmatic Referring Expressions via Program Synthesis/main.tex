\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{example.pdf}
\vspace{-20pt}
\caption{Left: An example image from the CLEVR dataset, and two referring relational programs that identify the target object G. The challenge is distinguishing G from the second gray cube H. The first program identifies the target object as the ``gray object in front of the brown object'', where the brown object is the sphere F. The second program identifies the target object as the ``gray object in front of the cube''. In this case, the cube can be either B or H, but either of these choices uniquely identifies G. Right: A challenging problem instance that requires several relations to solve (especially when restricted to three free variables---i.e., $|\mathcal{Z}|=3$). The program shown is generated by our algorithm. It identifies the target object as ``the large blue cube to the left of the object to the left of a large blue rubber cube''.}
\label{fig:example}
\end{figure*}

\section{Introduction}

Incorporating symbolic reasoning with deep neural networks (DNNs) is an important challenge in machine learning. Intuitively, DNNs are promising techniques for processing perceptual information; then, symbolic reasoning should be able to operate over the outputs of the DNNs to accomplish more abstract tasks. Recent work has successfully applied this approach to question-answering tasks, showing that leveraging programmatic representations can substantially improve performance---in particular, in visual question answering, by building a programmatic representation of the question and a symbolic representation of the image, we can evaluate the question representation in the context of the image representation to compute the answer~\cite{yi2018neural,mao2019neuro,ma2019towards}.

A natural question is whether incorporating symbolic reasoning with DNNs can be useful for tasks beyond question answering. In particular, consider the problem of generating a \emph{referring expression}---i.e., an image caption that uniquely identifies a given \emph{target object} in a given image~\cite{golland2010game,kazemzadeh2014referitgame}. In contrast to visual question answering, where we translate a question to a program and then execute that program, in this case, we want to \emph{synthesize} a program identifying the target object and then translate this program into a caption.

In this paper, we take a first step towards realizing this approach. In particular, we study the problem of generating a programmatic variant of referring expressions that we call \emph{referring relational programs}. We assume we are given a symbolic representation of an image---such a representation can be easily constructed using state-of-the-art deep learning algorithms~\cite{redmon2016you,krishna2017visual,yi2018neural,mao2019neuro}---together with a target object in that image. Then, our goal is to synthesize a relational program that uniquely identifies the target object in terms of its attributes and its relations to other objects in the image. Figure~\ref{fig:example} (left) shows an example of an image from the CLEVR dataset, and two referring relational programs for object G in this image. The task for this image is challenging since G has identical attributes as H, which means that the program must use spatial relationships to distinguish them.

Our formulation of referring relational programs can take into account the uncertainty in the predictions of the DNN used to construct the symbolic representation of the image. One approach would be to use probabilistic reasoning, but doing so can be computationally intractable. Instead, we use an approach based on uncertainty sets---we construct uncertainty sets that include all DNN predictions above a certain probability threshold, and require that the referring relational program uniquely identify the target object for \emph{all} possible configurations in these uncertainty sets.

Based on this formulation, we propose an algorithm for synthesizing referring relational programs
%\aws{typo -- not sure how sentence should end}
given the symbolic representation of the image. Fundamentally, program synthesis is a combinatorial search problem. The objective is to search over the space of possible programs to find one that achieves the given goal---in our case, find a relational program that uniquely identifies the target object when evaluated against the symbolic representation of the image.

To account for the combinatorial size of the search space, our synthesis algorithm builds on recent techniques for speeding up program synthesis. First, it leverages \emph{execution-guided synthesis}~\cite{chen2018execution}, where deep learning is used to guide the search over the space of programs. This approach formulates the search problem as a Markov decision process, where actions correspond to decisions about which statements to include in the program, and states correspond to the intermediate program state obtained by incrementally evaluating the program generated so far. Then, it uses deep reinforcement learning to solve the synthesis problem. In particular, we use deep $Q$-learning, where the $Q$-network is a graph convolutional network (GCN) that takes as input a graph encoding of the program state; then, the $Q$-value for each action is evaluated based on local representations, avoiding the use of a lossy global representation.

Second, we leverage \emph{hierarchical synthesis}~\citep{nye2019learning}, where the neurosymbolic synthesizer is combined with a faster but unguided enumerative synthesizer---intuitively, the neurosymbolic synthesizer generates the majority of the program. Then, the enumerative synthesizer fills in the remainder of the program, which tends to be a smaller but more challenging search problem. Finally, we use a simple \emph{meta-learning} approach~\cite{si2018learning}, where the $Q$-network is pretrained on a benchmark of training synthesis tasks; this $Q$-network is used to initialize the $Q$-networks for solving future synthesis tasks.

We evaluate our approach on the CLEVR dataset~\cite{johnson2017clevr}, a synthetic dataset of objects with different attributes and spatial relationships. Our goal is to generate a relational program that identifies one of the objects in the scene. We consider both synthetic examples where the ground truth scene graph is known, as well as cases where the scene graph is predicted using a convolutional neural network (CNN) and may be prone to error.

We leverage control over the data generation process to generate problem instances that are particularly challenging---i.e., where there are multiple objects with the same attributes in each scene. By doing so, a valid relational referring program must include
%\aws{too much "leverage"}
complex spatial relationships to successfully identify the target object. We demonstrate how our approach outperforms several baselines, including a state-of-the-art program synthesizer~\cite{si2018learning}.

%Problem formulation (**) \\ 
%Motivating example \\
%Defining the language \\ 
%Keep it more intuitive rather than formal notation \\
%Don't need to define the syntax too much. Give examples. 

%1. Overview describe of problem \\
%2. Reference through out text \\
%3. Define language & environment  \\

%Formulation as an RL problem, action, state, transition, reward, algorithm.  \\
%Spec: Goal - shortest path program uniquely identify the object. Set of the object to the scene that the variable could bind to. \\ 

%The experiment is of higher priority. 
%Tip for Latex: Macro for Notation. \\

\textbf{Related work.}
%
There has been a great deal of recent interest in leveraging program synthesis to improve machine learning---e.g., to classify images based on their parts~\cite{lake2015human}, to infer the structure of images~\cite{ellis2015unsupervised,ellis2018learning,pu2018selecting}, to perform procedural tasks over images~\cite{gaunt2017differentiable,valkov2018houdini}, to generate images with programmatic structure~\cite{young2019learning}, and interpretable and robust reinforcement learning~\cite{verma2018programmatically,bastani2018verifiable,verma2019imitation,jothimurugan2019composable,inala2020synthesizing}. These techniques demonstrate how incorporating program synthesis into machine learning tasks can improve performance on tasks that involve programmatic or symbolic reasoning.

Most closely related, there has been interest in leveraging programmatic reasoning in the domain of visual question answering~\cite{mao2019neuro,ma2019towards}. These approaches translate the question into a program using semantic parsing, and translate the image into a symbolic representation (e.g., a scene graph encoding the object attributes and relationships); then, they execute the program in the context of the symbolic representation as input to produce the answer to the given question.

Whereas visual question answering corresponds to \emph{executing} a program in the context of the scene graph, our key insight is that generating a referring expression corresponds to \emph{synthesizing} a program that, when executed in the context of the scene graph, produces the given target object.

There has been work on incorporating logical reasoning into deep neural networks to perform reasoning tasks such as sorting and shortest path~\cite{dong2019neural} or solving Sudoku problems~\cite{wang2019satnet}, including work incorporating relational reasoning into deep neural networks to improve question answering~\cite{santoro2017simple} and planning~\cite{santoro2018relational}, as well as general frameworks incorporating relational programs with probabilistic inference~\cite{de2007problog} or deep learning~\cite{cohen2018tensorlog,manhaeve2018deepproblog}. In contrast, our goal is to \emph{generate} relational programs to achieve some goal.

There has been work on synthesizing relational programs~\cite{albarghouthi2017constraint,si2018syntax,raghothaman2019provenance,si2019synthesizing}, though this work focuses on relational programs with different structure than ours. In particular, they typically assume the space of possible rules is not too large, and the goal is to find the right combination of rules; in contrast, our goal is to find a single rule in a combinatorially large search space of rules. There has also been work on using machine learning to speed up synthesis~\cite{menon2013machine,balog2017deepcoder,parisotto2017neuro,bunel2018leveraging,feng2018program,ellis2018dreamcoder}; which we leverage in our algorithm.

\input{semantics}
\input{procedure}
\input{synthesis}

\section{Experiments}

\begin{figure*}[t]
\centering
\includegraphics[width=0.4\textwidth]{baselines.pdf}
\hspace{0.5in}
\includegraphics[width=0.4\textwidth]{ablations.pdf}
\caption{Fraction of problem instances solved in a variety of benchmarks by different algorithms. Left: Comparing our algorithm (black) to baselines neurosymbolic synthesis (blue) and enumerative synthesis with $M=3$ (red). Right: Comparing our algorithm (black) to ablations without hierarchical synthesis (green) and without execution guidance (yellow).} %\aws{would be nice to have a legend---it's atypical to have colors in only in caption}
\label{fig:baselines}
\end{figure*}

%We first discuss the datasets we evaluate on. Followed with the comparison of the implementation details between our model and baselines, and then we show an ablation study on the performance, generalizability, and robustness across the models. 

We evaluate our approach on the CLEVR dataset, both (i) on purely synthetic graphs that we generated, and (ii) on graphs constructed using a CNN based on the synthetic images in the dataset. As we discuss below, we focus on synthetic data since it allows us to generate challenging problem instances that require the use of relationships involving multiple objects (in this case, spatial relationships). Based on these datasets, we demonstrate that our approach outperforms several baselines and ablations.

\subsection{Experimental Setup}

\textbf{Synthetic graphs.}
%
The first dataset is a set of synthetic scene graphs that include objects and relations between these objects, including unary ones (called \emph{attributes}), namely shape, color, and material, as well as binary ones that encode spatial relations, namely front/behind and left/right.

Using this approach, we can create challenging problem instances (i.e., a scene graph and a target object) to evaluate our algorithm. Our primary goal is to create problem instances where the referring relational program has to include spatial relationships to identify the target object. These instances are challenging since multiple relations are needed to distinguish two identical objects---e.g., in Figure~\ref{fig:example} (left), at least two relations are needed to distinguish G from H, and more are needed in Figure~\ref{fig:example} (right).

To this end, we create graphs with multiple identical objects in the scene. We classify these datasets by the set of counts of identical objects (in terms of attributes). For instance, the dataset CLEVR-4-3 consists of 7 objects total, the first 4 and last 3 of which have identical attributes---e.g., it might contain 4 gray metal cubes and 3 red metal spheres.

For simplicity, we directly generate scene graphs; thus, they do not contain any uncertainty. We impose constraints on the graphs to ensure they can be rendered into actual CLEVR images if desired. We consider the following datasets: 3-1-1-1-1, 4-1-1-1, 5-1-1, 6-1, 5-2, and 4-3. For each dataset, we use 7 total objects. Each dataset has 30 scene graphs for training (a total of 210 problem instances), and 500 scene graphs for testing (a total of 3500 problem instances).

\textbf{CLEVR images.}
%
We also evaluate based on a dataset of images from the original CEVR dataset. These images have the same kinds of relations as our generated scene graphs.

We use a convolutional neural network (CNN) to construct the scene graph~\cite{yi2018neural}. For simplicity, this CNN predicts both object attributes and positions. The object attributes are predicted independently---i.e., it could predict that object J is both red with probability $0.75$ and purple with probability $0.75$. We consider a relation to be absent if $p=p(\rho(o_1,...,o_n))<1/2$; for relations with $p\ge1/2$, we consider them to be uncertain if there are multiple such attributes of the type (e.g., object J is predicted to be both red and purple with probability $\ge1/2$), and certain otherwise. The spatial relationships are inferred based on the object positions; we consider it to be uncertain if the objects are very close together along some dimension.

\textbf{Our algorithm.}
%
We search for programs of length at most $M=8$, using $K=2$ in hierarchical synthesis. We consider three variables---i.e., $|\mathcal{Z}|=3$, including $\tvar$. We use $N=200$ rollouts during reinforcement learning. We pretrain our $Q$-network on the training set corresponding to each dataset, using $N=10000$ gradient steps with a batch size of $5$.

\subsection{Comparison to Baselines}

We use each algorithm on our synthetic graphs dataset; in Figure~\ref{fig:baselines} (left), we report what fraction of each kind of problem instance that is solved by each one.

\textbf{Neurosymbolic synthesis.}
%
We compare to a state-of-the-art synthesizer called Metal~\cite{si2018learning}. This approach uses reinforcement learning to find a program that satisfies the given specification; in addition, they use the same simple meta-learning approach as ours. As can be seen in Figure~\ref{fig:baselines}, our approach substantially outperforms this baseline by using hierarchical synthesis and execution-guided synthesis. For instance, on 6-1, our approach solves 82\%; in contrast, Metal solves just 19\%. Similarly, on 4-3, our approach solves 97\% whereas Metal solves just 6\%. We believe Metal works poorly in our setting due to the lack of intermediate feedback in our setting.

\textbf{Enumerative synthesis.}
%
We compare to a synthesis algorithm that enumerates programs to find one that solves the task~\cite{alur2013syntax}. This approach does not use machine learning to guide its search, making it challenging to scale to large programs (i.e., large $M$) due to the combinatorial blowup in the search space; thus, we consider $M=3$. As can be seen in Figure~\ref{fig:baselines}, our approach substantially outperforms enumerative synthesis. For instance, on 6-1, our approach solves 82\%, whereas enumerative synthesis solves 42\%, and on 4-3, our approach solves 97\%, whereas enumerative synthesis solves 41\%.

%\textbf{Greedy algorithm.}
%
%Finally, we compare to an algorithm that greedily selects the most promising relations to include based on how many assignments $\tvar\to o$ they remove, where $o$ is an object in $G$. Intuitively, this approach performs best when attributes are sufficient to distinguish the target object, since in this case reasoning about multiple relations is unnecessary. As can be seen in Figure~\ref{fig:baselines}, our algorithm substantially outperforms this approach, especially on the more challenging problem instances.

\subsection{Comparison to Ablations}

We compare to two ablations on the synthetic graph datasets; in Figure~\ref{fig:baselines} (right), we report what fraction of the benchmarks in each category are solved.

\textbf{Hierarchical synthesis.}
%
Next, we compare to an ablation that does not use hierarchical synthesis---i.e., it only count the program generated by the neural symbolic synthesizer, but no enumerative search to correct the generated program. As can be seen, our approach substantially outperforms this ablation---e.g., on 6-1, hierarchical synthesis improves performance from 35\% to 82\%, and on 4-3, it improves performance from 46\% to 97\%. Intuitively, hierarchical synthesis improves performance by using reinforcement learning to find the larger but more straightforward parts of the program, whereas the enumerative synthesizer can find the more challenging parts using brute force.

\textbf{Execution guidance.}
%
We compare to an ablation where we do not use the interpreter to guide RL; instead, the states are partial programs $P$. In particular, this ablation does not have feedback from the interpreter until the sparse reward at the very end of a rollout. As can be seen from Figure~\ref{fig:baselines}, using execution guidance improves our performance, especially on harder benchmarks---e.g., for the 6-1 benchmark, it improves performance from 72\% to 82\%.

\subsection{CLEVR Images}

We tested our approach on CLEVR images where we constructed the scene graph using a CNN~\cite{yi2018neural}. Out of 6487 tasks, our approach solved all but 25 according to the ground truth relations---i.e., the ground truth in the CLEVR dataset, not the predicted ones seen by our interpreter. Thus, our approach works well even when there is uncertainty in the scene graph predicted using a CNN.

%Other than the DQN model, we have explored quite a few different models, encoding and decoding strategies. We evaluate the performance on a handcrafted unit test, with one scene and 3 objects. We define passing the unit test as the model can overfit to the dataset in 10000 gradient steps. 

%The GNN converts a graph into a matrix of node representations. The common practice utilizing the latent matrix is to combine them to one single global representation and decode from the latent vector. We propose a different strategy called local action decoding. We select the corresponding nodes which have a direct impact on the action along with the global representation and pass these embeddings through a neural network to obtain scores for the exact action. Then we pick the corresponding action based on the scores. 
%$s = \pi mask(M)$
%\begin{align*}
%s &= \pi_{\theta} \textit {mask} (M)
%\end{align*}

%We evaluate the performance of a unit test scene with three objects. We compare the performance based on the training error. If the model cannot overfit to the simple unit test, it means the model cannot learn well. 

%Utilizing the local action decoding strategy better captures the local information than utilizing the global representation alone. We compared the performance over three different ways to obtain the global embedding and the local action decoding method under the policy gradient reinforcement learning algorithm. The global embeddings are obtained by aggregation, attention layer, and add a global node in the graph. Regardless of how the global embedding is generated, we found that only the local action decoding strategy passes the unit test.

%By adding the interpreter in the learning loop, we can provide further execution guidance to the learning process: rather than obtaining the reward only at the end of rollouts, we have intermediate reward for each clause taken; the states are unified for partial programs with the same effects but different expressions; we also have a wider range of reinforcement learning algorithm to choose from, as Q-learning algorithms requires separable states.

%We compare the performance of the models by the success rate. In terms of the gradient-based methods, our method significantly outperforms the LSTM baseline, especially when the test dataset difficulty increases. Although the enumerative method performs well in terms of accuracy, it has an exponential time complexity and query number as the depth grows. Therefore, we use the enumerative method to evaluate the difficulty of the dataset and the capability of the neural network models. 

%We consider the enumerative approach as the most general method and use the enumerative approach to denote the difficulty of the datasets. For the DQN method, the accuracy falls mostly in the Enum-d3 range, while the LSTM method is approximately Enum-d2. By further comparing the DQN + Enum 1 and LSTM + Enum 1, our method also demonstrates better generalizability on the harder datasets. 

%\subsection{Optimization}
%\subsection{Dataset}

\section{Conclusions}

We have proposed an approach to solving a symbolic variant of referring expressions using program synthesis. Our work is a first step towards incorporating symbolic reasoning into image captioning tasks. Future work includes leveraging our approach to generate natural language referring expressions for real-world images---i.e., by synthesizing a referring relational program and translating it to natural language. In addition, we have ignored the problem of \emph{naturalness} for the programs we generate (i.e., how easy it is for a human to understand the program), since we are focused on cases where complex logical reasoning is needed; in practice, naturalness is an important criterion that must be addressed.